{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱회귀모델 with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이타 불러오기: 전처리가 된 텍스트 파일\n",
    "- wrod2vec은 단어로 표현된 리스트를 입력값으로 넣어야 하기 때문데 \n",
    "- 전처리된 텍스트 csv 파일을 불러온 후 각 단어들의 리스트로 나눠야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec을 사용하기 위해 입력값을 단어로 구분된 리스트로 만들어야 한다.\n",
    "- 읽어온 학습 데이타 전체 리뷰를 단어 리스트로 바꾼다.\n",
    "- 각 리뷰를 split 함수를 이용해서 띄어쓰기 기준으로 구분한 후 리스트에 하나씩 추가해서 단어리스트를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stuff',\n",
       "  'going',\n",
       "  'moment',\n",
       "  'mj',\n",
       "  'started',\n",
       "  'listening',\n",
       "  'music',\n",
       "  'watching',\n",
       "  'odd',\n",
       "  'documentary',\n",
       "  'watched',\n",
       "  'wiz',\n",
       "  'watched',\n",
       "  'moonwalker',\n",
       "  'maybe',\n",
       "  'want',\n",
       "  'get',\n",
       "  'certain',\n",
       "  'insight',\n",
       "  'guy',\n",
       "  'thought',\n",
       "  'really',\n",
       "  'cool',\n",
       "  'eighties',\n",
       "  'maybe',\n",
       "  'make',\n",
       "  'mind',\n",
       "  'whether',\n",
       "  'guilty',\n",
       "  'innocent',\n",
       "  'moonwalker',\n",
       "  'part',\n",
       "  'biography',\n",
       "  'part',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'remember',\n",
       "  'going',\n",
       "  'see',\n",
       "  'cinema',\n",
       "  'originally',\n",
       "  'released',\n",
       "  'subtle',\n",
       "  'messages',\n",
       "  'mj',\n",
       "  'feeling',\n",
       "  'towards',\n",
       "  'press',\n",
       "  'also',\n",
       "  'obvious',\n",
       "  'message',\n",
       "  'drugs',\n",
       "  'bad',\n",
       "  'kay',\n",
       "  'visually',\n",
       "  'impressive',\n",
       "  'course',\n",
       "  'michael',\n",
       "  'jackson',\n",
       "  'unless',\n",
       "  'remotely',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'anyway',\n",
       "  'going',\n",
       "  'hate',\n",
       "  'find',\n",
       "  'boring',\n",
       "  'may',\n",
       "  'call',\n",
       "  'mj',\n",
       "  'egotist',\n",
       "  'consenting',\n",
       "  'making',\n",
       "  'movie',\n",
       "  'mj',\n",
       "  'fans',\n",
       "  'would',\n",
       "  'say',\n",
       "  'made',\n",
       "  'fans',\n",
       "  'true',\n",
       "  'really',\n",
       "  'nice',\n",
       "  'actual',\n",
       "  'feature',\n",
       "  'film',\n",
       "  'bit',\n",
       "  'finally',\n",
       "  'starts',\n",
       "  'minutes',\n",
       "  'excluding',\n",
       "  'smooth',\n",
       "  'criminal',\n",
       "  'sequence',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  'convincing',\n",
       "  'psychopathic',\n",
       "  'powerful',\n",
       "  'drug',\n",
       "  'lord',\n",
       "  'wants',\n",
       "  'mj',\n",
       "  'dead',\n",
       "  'bad',\n",
       "  'beyond',\n",
       "  'mj',\n",
       "  'overheard',\n",
       "  'plans',\n",
       "  'nah',\n",
       "  'joe',\n",
       "  'pesci',\n",
       "  'character',\n",
       "  'ranted',\n",
       "  'wanted',\n",
       "  'people',\n",
       "  'know',\n",
       "  'supplying',\n",
       "  'drugs',\n",
       "  'etc',\n",
       "  'dunno',\n",
       "  'maybe',\n",
       "  'hates',\n",
       "  'mj',\n",
       "  'music',\n",
       "  'lots',\n",
       "  'cool',\n",
       "  'things',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'turning',\n",
       "  'car',\n",
       "  'robot',\n",
       "  'whole',\n",
       "  'speed',\n",
       "  'demon',\n",
       "  'sequence',\n",
       "  'also',\n",
       "  'director',\n",
       "  'must',\n",
       "  'patience',\n",
       "  'saint',\n",
       "  'came',\n",
       "  'filming',\n",
       "  'kiddy',\n",
       "  'bad',\n",
       "  'sequence',\n",
       "  'usually',\n",
       "  'directors',\n",
       "  'hate',\n",
       "  'working',\n",
       "  'one',\n",
       "  'kid',\n",
       "  'let',\n",
       "  'alone',\n",
       "  'whole',\n",
       "  'bunch',\n",
       "  'performing',\n",
       "  'complex',\n",
       "  'dance',\n",
       "  'scene',\n",
       "  'bottom',\n",
       "  'line',\n",
       "  'movie',\n",
       "  'people',\n",
       "  'like',\n",
       "  'mj',\n",
       "  'one',\n",
       "  'level',\n",
       "  'another',\n",
       "  'think',\n",
       "  'people',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'try',\n",
       "  'give',\n",
       "  'wholesome',\n",
       "  'message',\n",
       "  'ironically',\n",
       "  'mj',\n",
       "  'bestest',\n",
       "  'buddy',\n",
       "  'movie',\n",
       "  'girl',\n",
       "  'michael',\n",
       "  'jackson',\n",
       "  'truly',\n",
       "  'one',\n",
       "  'talented',\n",
       "  'people',\n",
       "  'ever',\n",
       "  'grace',\n",
       "  'planet',\n",
       "  'guilty',\n",
       "  'well',\n",
       "  'attention',\n",
       "  'gave',\n",
       "  'subject',\n",
       "  'hmmm',\n",
       "  'well',\n",
       "  'know',\n",
       "  'people',\n",
       "  'different',\n",
       "  'behind',\n",
       "  'closed',\n",
       "  'doors',\n",
       "  'know',\n",
       "  'fact',\n",
       "  'either',\n",
       "  'extremely',\n",
       "  'nice',\n",
       "  'stupid',\n",
       "  'guy',\n",
       "  'one',\n",
       "  'sickest',\n",
       "  'liars',\n",
       "  'hope',\n",
       "  'latter']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델의 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_features : 각 단어에 대한 임베딩된 벡터의 차원 설정\n",
    "- min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도수의 단어들은 학습하지 않는다.\n",
    "- num_workers : 모델 학습 시 학습을 위한 프로세스의 개수를 지정\n",
    "- context : word2vec을 수행하기 위한 컨텍스트 윈도우 크기를 지정\n",
    "- downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운 샘플링 비율을 지정\n",
    "    - 보통 0.001이 좋은 성능을 낸다고 하낟."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # 워드 벡터 특징값 수\n",
    "min_word_count = 40   # 단어데 대한 최소 빈도수 \n",
    "num_workers = 4       # 프로세스 개수\n",
    "context = 10          # 컨텍스트 윈도 크기\n",
    "downsampling = 1e-3   # 다운 샘플링 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 설정된 하이퍼파라미터를 가지고 word2vec 학습\n",
    "- gensim 라이브러리 사용, gensim.models에 있는 word2vec 모듈 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 학습하는 과정에서 진행 상황을 확인하기 위해 logging 사용\n",
    "- level(로그 수준)를 INFO에 맞추면 word2vec 학습과정에서 로그메세지를 양식에 맞게 INFO 수준으로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 학습\n",
    "- word2vec 모듈에 있는 Word2Vec 객체를 생성하여 실행\n",
    "- 학습하고 생성된 객체는 model 변수에 할당\n",
    "- 입력데이타와 하이퍼파라미터를 순서대로 입력하여 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 11:20:15,495 : INFO : collecting all words and their counts\n",
      "2021-03-04 11:20:15,499 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-04 11:20:16,015 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-03-04 11:20:16,520 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-03-04 11:20:16,714 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-03-04 11:20:16,715 : INFO : Loading a fresh vocabulary\n",
      "2021-03-04 11:20:16,808 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2021-03-04 11:20:16,808 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2021-03-04 11:20:16,867 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-03-04 11:20:16,867 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2021-03-04 11:20:16,882 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2021-03-04 11:20:16,922 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2021-03-04 11:20:16,922 : INFO : resetting layer weights\n",
      "2021-03-04 11:20:20,983 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-03-04 11:20:22,049 : INFO : EPOCH 1 - PROGRESS: at 15.33% examples, 372702 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:23,038 : INFO : EPOCH 1 - PROGRESS: at 31.20% examples, 384514 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:24,049 : INFO : EPOCH 1 - PROGRESS: at 47.48% examples, 391041 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:25,058 : INFO : EPOCH 1 - PROGRESS: at 64.00% examples, 395455 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:26,092 : INFO : EPOCH 1 - PROGRESS: at 79.41% examples, 390937 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:27,101 : INFO : EPOCH 1 - PROGRESS: at 95.78% examples, 392778 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:27,264 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 11:20:27,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 11:20:27,315 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 11:20:27,324 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 11:20:27,324 : INFO : EPOCH - 1 : training on 2988089 raw words (2494516 effective words) took 6.3s, 394628 effective words/s\n",
      "2021-03-04 11:20:28,394 : INFO : EPOCH 2 - PROGRESS: at 14.66% examples, 355530 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:29,406 : INFO : EPOCH 2 - PROGRESS: at 31.20% examples, 381444 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:30,417 : INFO : EPOCH 2 - PROGRESS: at 47.78% examples, 390668 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:31,455 : INFO : EPOCH 2 - PROGRESS: at 63.67% examples, 389050 words/s, in_qsize 8, out_qsize 1\n",
      "2021-03-04 11:20:32,471 : INFO : EPOCH 2 - PROGRESS: at 80.47% examples, 392906 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:33,480 : INFO : EPOCH 2 - PROGRESS: at 97.45% examples, 397082 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:33,589 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 11:20:33,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 11:20:33,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 11:20:33,641 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 11:20:33,641 : INFO : EPOCH - 2 : training on 2988089 raw words (2494539 effective words) took 6.3s, 396838 effective words/s\n",
      "2021-03-04 11:20:34,665 : INFO : EPOCH 3 - PROGRESS: at 15.00% examples, 378565 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:35,665 : INFO : EPOCH 3 - PROGRESS: at 31.20% examples, 391887 words/s, in_qsize 6, out_qsize 1\n",
      "2021-03-04 11:20:36,662 : INFO : EPOCH 3 - PROGRESS: at 47.48% examples, 395433 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:37,676 : INFO : EPOCH 3 - PROGRESS: at 62.64% examples, 391865 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:38,685 : INFO : EPOCH 3 - PROGRESS: at 79.41% examples, 394795 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:39,739 : INFO : EPOCH 3 - PROGRESS: at 96.41% examples, 396347 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:39,854 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 11:20:39,908 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 11:20:39,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 11:20:39,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 11:20:39,921 : INFO : EPOCH - 3 : training on 2988089 raw words (2494235 effective words) took 6.3s, 398361 effective words/s\n",
      "2021-03-04 11:20:40,959 : INFO : EPOCH 4 - PROGRESS: at 15.00% examples, 373854 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:41,961 : INFO : EPOCH 4 - PROGRESS: at 31.20% examples, 389548 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:42,979 : INFO : EPOCH 4 - PROGRESS: at 47.78% examples, 393430 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:43,991 : INFO : EPOCH 4 - PROGRESS: at 64.40% examples, 398649 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:44,999 : INFO : EPOCH 4 - PROGRESS: at 79.41% examples, 393234 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:45,997 : INFO : EPOCH 4 - PROGRESS: at 95.45% examples, 393281 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:46,200 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 11:20:46,224 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 11:20:46,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 11:20:46,262 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 11:20:46,267 : INFO : EPOCH - 4 : training on 2988089 raw words (2494966 effective words) took 6.3s, 394619 effective words/s\n",
      "2021-03-04 11:20:47,296 : INFO : EPOCH 5 - PROGRESS: at 15.95% examples, 400429 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:48,310 : INFO : EPOCH 5 - PROGRESS: at 32.82% examples, 407955 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:49,314 : INFO : EPOCH 5 - PROGRESS: at 48.39% examples, 401136 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:50,314 : INFO : EPOCH 5 - PROGRESS: at 64.00% examples, 398017 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:51,327 : INFO : EPOCH 5 - PROGRESS: at 79.07% examples, 391946 words/s, in_qsize 7, out_qsize 0\n",
      "2021-03-04 11:20:52,384 : INFO : EPOCH 5 - PROGRESS: at 96.07% examples, 393610 words/s, in_qsize 8, out_qsize 0\n",
      "2021-03-04 11:20:52,572 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-03-04 11:20:52,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-04 11:20:52,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-04 11:20:52,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-04 11:20:52,592 : INFO : EPOCH - 5 : training on 2988089 raw words (2494113 effective words) took 6.3s, 395475 effective words/s\n",
      "2021-03-04 11:20:52,597 : INFO : training on a 14940445 raw words (12472369 effective words) took 31.6s, 394658 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "           size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec으로 학습된 모델 저장\n",
    "- 모델 저장 model.save(모델이름) \n",
    "- 저장한 모델은 Word2Vec.load()로 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-04 11:23:48,353 : INFO : saving Word2Vec object under ./data_out/300features_40minwords_10context, separately None\n",
      "2021-03-04 11:23:48,354 : INFO : not storing attribute vectors_norm\n",
      "2021-03-04 11:23:48,354 : INFO : not storing attribute cum_table\n",
      "2021-03-04 11:23:48,780 : INFO : saved ./data_out/300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./data_out/300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 모델 적용하기 위해 전처리\n",
    "- 하나의 리뷰를 같은 형태의 입력값으로 만들어야 한다.\n",
    "- word2vec 모델에서 각 단어가 벡터로 표현되어 있고, 리뷰마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.\n",
    "- 가장 단순한 방법 : 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터를 만드는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나의 리뷰에 대해 전체 단어의 평균값을 계산하는 함수\n",
    "- 문장에 특징값을 만들 수 있는 함수\n",
    "- get_features(단어의 모음인 하나의 리뷰, word2vec모델, word2vec으로 임베딩할 때 정했던 벡터의 차원수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    \n",
    "    # np.zeros 함수를 사용해 모두 0의 값을 가지는 벡터 생성\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
    "\n",
    "    # 문장이 단어가 해당 모델 단어 사전에 속하는지 확인하기 위해 set 객체로 생성\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    # 리뷰를 구성하는 단어에 대해 임베딩된 벡터가 있는 단어 벡터의 합을 구한다.\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "\n",
    "    # 사용한 단어의 전체 개수로 나눠서 평균 벡터의 값을 구한다.\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수\n",
    "- get_dataset(학습데이타의 전체 리뷰데이타, word2vec모델, word2vec으로 임베딩할때 정했던 벡터의 차원수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 리뷰에 대해 평균 벡터 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jinsuk kim\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00970184e-02, -6.70368820e-02,  5.29728085e-02,\n",
       "        -2.06597373e-02,  1.43107489e-01,  2.62437314e-01,\n",
       "         1.10805079e-01,  1.08345069e-01,  1.21532241e-02,\n",
       "        -1.30330294e-01,  1.15363471e-01, -2.41566628e-01,\n",
       "         3.98410857e-02, -3.14722359e-01,  1.60251498e-01,\n",
       "         4.45713811e-02,  9.25093237e-03,  7.60926977e-02,\n",
       "        -1.76173285e-01, -2.33845741e-01,  6.36781454e-02,\n",
       "         3.10567953e-03,  4.58594933e-02, -3.45952883e-02,\n",
       "        -5.00632487e-02,  6.29462972e-02,  1.31065175e-01,\n",
       "        -1.88865792e-02, -1.81590754e-03, -1.89025894e-01,\n",
       "        -1.21760480e-01,  1.64418355e-01, -3.23844284e-01,\n",
       "         9.73522514e-02,  1.10095670e-03,  1.44492805e-01,\n",
       "        -7.20995246e-03, -7.71756396e-02,  3.38824093e-02,\n",
       "        -7.24842176e-02,  1.57285526e-01, -5.89011125e-02,\n",
       "         3.37456097e-03,  3.62957940e-02, -2.89626479e-01,\n",
       "         1.42615542e-01, -1.85335234e-01, -1.67775080e-02,\n",
       "         1.44413039e-01, -6.17982773e-03,  2.15693265e-01,\n",
       "         1.04688928e-01, -4.99173217e-02,  2.49139771e-01,\n",
       "         1.83311794e-02,  2.57325888e-01,  5.15247956e-02,\n",
       "        -2.00950637e-01, -9.94600076e-03, -5.48072532e-03,\n",
       "        -8.91832486e-02, -1.05376959e-01, -8.05122107e-02,\n",
       "        -4.21051644e-02, -3.69886816e-01, -3.84236611e-02,\n",
       "        -6.15510456e-02, -9.37208608e-02, -3.00230354e-01,\n",
       "        -6.18428178e-02,  1.44213429e-02, -3.15374136e-01,\n",
       "        -4.91757393e-02, -9.76188034e-02, -4.77621816e-02,\n",
       "         1.86192747e-02,  5.65101616e-02, -1.75967410e-01,\n",
       "        -4.45491523e-02,  6.72822744e-02,  2.15106115e-01,\n",
       "        -1.33567527e-01, -7.23470971e-02, -1.32866696e-01,\n",
       "        -2.21200213e-01,  2.58739442e-01,  1.29226735e-02,\n",
       "         2.66524643e-01, -1.30745530e-01, -1.79903045e-01,\n",
       "         8.15358236e-02,  3.99314873e-02, -3.10878735e-02,\n",
       "         1.35679364e-01, -1.14704808e-02,  2.53667384e-01,\n",
       "         1.39447257e-01, -1.29510567e-01, -3.79696004e-02,\n",
       "        -1.56487405e-01, -2.73332167e-02, -1.22081950e-01,\n",
       "        -2.44225949e-01,  9.51092169e-02, -1.51243746e-01,\n",
       "         2.61717260e-01,  2.01895967e-01,  7.12507442e-02,\n",
       "        -1.33640438e-01, -1.30079873e-02, -7.17989802e-02,\n",
       "         1.17075346e-01,  3.04473266e-02,  7.56630953e-03,\n",
       "        -1.65738434e-01,  2.38251016e-02,  2.00208202e-02,\n",
       "         2.28894819e-02,  1.56407177e-01, -1.36042461e-01,\n",
       "         1.10217132e-01,  1.20563418e-01, -1.29094332e-01,\n",
       "         1.07176527e-01, -8.84665549e-02, -1.31412353e-02,\n",
       "         8.56239945e-02,  1.43169612e-01, -2.47240514e-01,\n",
       "         2.92595029e-02, -1.50267174e-02, -1.34444192e-01,\n",
       "        -1.41253665e-01, -1.49861604e-01, -1.59093380e-01,\n",
       "        -3.25956881e-01,  1.56921923e-01, -7.29442462e-02,\n",
       "        -2.27518722e-01,  2.25448713e-01,  2.90374547e-01,\n",
       "         4.58476916e-02,  1.96125105e-01,  1.62219569e-01,\n",
       "        -1.53813556e-01, -2.12812036e-01,  2.51080636e-02,\n",
       "        -1.23598263e-01, -1.86634913e-01, -1.34618625e-01,\n",
       "         8.81004706e-02,  3.44439670e-02,  3.06156315e-02,\n",
       "         6.94064610e-03, -1.18291192e-01, -1.19713604e-01,\n",
       "        -1.55554697e-01, -4.10616361e-02, -2.25060910e-01,\n",
       "         7.14321807e-03,  1.26430079e-01, -1.27199963e-01,\n",
       "         1.34736896e-01,  2.37445403e-02,  7.25768879e-02,\n",
       "        -1.52395278e-01,  2.90834773e-02,  1.07066162e-01,\n",
       "         1.78947866e-01, -1.28273875e-01, -4.16857786e-02,\n",
       "        -1.25341833e-01,  3.24988179e-02, -8.47573131e-02,\n",
       "         4.73191999e-02,  5.81401326e-02,  1.80368587e-01,\n",
       "         2.17472523e-01,  1.57982528e-01,  1.07881457e-01,\n",
       "         1.27493590e-01,  1.36694103e-01,  1.58196449e-01,\n",
       "         4.97194111e-01, -2.65014470e-01, -1.08106986e-01,\n",
       "         9.21962503e-03, -2.12081924e-01, -6.14639223e-02,\n",
       "        -7.63479248e-02, -2.54447982e-02, -2.58402452e-02,\n",
       "         2.53691703e-01,  2.20201351e-02,  1.04538158e-01,\n",
       "         1.22032449e-01, -2.22033967e-04, -2.06636507e-02,\n",
       "         1.28805667e-01, -1.41550168e-01, -1.93869233e-01,\n",
       "        -6.73049688e-02,  3.16539556e-02, -5.70212826e-02,\n",
       "        -1.29966229e-01,  6.53270856e-02,  1.70964450e-01,\n",
       "        -1.10997871e-01, -9.13233310e-02,  5.68272173e-02,\n",
       "         1.49202153e-01,  2.11700976e-01,  1.10168256e-01,\n",
       "         7.14345947e-02,  1.24129858e-02, -4.36260216e-02,\n",
       "         2.88611561e-01, -2.33628571e-01,  3.45160544e-01,\n",
       "         2.00925708e-01, -2.54751116e-01, -2.84947641e-02,\n",
       "         1.00577056e-01,  3.97532284e-02, -4.99234796e-02,\n",
       "         1.12844661e-01,  2.21548062e-02,  2.90836059e-02,\n",
       "         7.92395473e-02, -1.80390567e-01,  3.57845351e-02,\n",
       "         5.84104098e-02, -1.80717573e-01,  3.88460867e-02,\n",
       "        -2.06585273e-01,  4.31437194e-01,  9.59207267e-02,\n",
       "         1.55325353e-01, -3.16997498e-01,  2.39919707e-01,\n",
       "         4.13634926e-02, -9.95258838e-02, -2.38244370e-01,\n",
       "         6.93858489e-02, -1.66092124e-02, -2.00807065e-01,\n",
       "         1.71676055e-01, -1.62801683e-01,  2.64195874e-02,\n",
       "        -2.03415900e-01,  2.30767459e-01, -1.55571532e-02,\n",
       "         5.83170988e-02, -2.44308308e-01,  6.48933987e-04,\n",
       "         8.52453858e-02, -1.18365981e-01,  3.96136761e-01,\n",
       "        -6.76163100e-03, -1.30626291e-01, -1.84044689e-01,\n",
       "        -2.06791431e-01, -2.38188073e-01,  1.21909723e-01,\n",
       "         3.72337811e-02, -1.65911660e-01,  3.72063927e-02,\n",
       "        -7.05968738e-02, -5.40456213e-02, -3.53450775e-02,\n",
       "         8.59133378e-02, -1.40698522e-01, -2.84312014e-02,\n",
       "        -2.39784375e-01,  2.00629622e-01,  8.71100351e-02,\n",
       "        -1.63772225e-01,  2.35840693e-01,  1.21581316e-01,\n",
       "        -9.22917202e-02, -6.52375445e-02,  2.31949985e-02,\n",
       "        -1.99592542e-02, -2.80995786e-01, -6.67896029e-03,\n",
       "        -2.04285979e-01, -1.00374162e-01,  1.58907752e-02,\n",
       "        -8.88420828e-03, -2.13877726e-02,  1.43387899e-01,\n",
       "         1.12124175e-01, -1.63766131e-01,  2.28264704e-02,\n",
       "         1.13706090e-01,  1.90321073e-01,  1.09459221e-01,\n",
       "        -2.81456970e-02, -3.22511233e-03,  1.71161190e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vecs[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이타와 테스트 데이타 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "# TEST_SPLIT = 0.2\n",
    "\n",
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jinsuk Kim\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측\n",
    "predicted = lgs.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 정답\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862400\n"
     ]
    }
   ],
   "source": [
    "# 정확도\n",
    "print(\"Accuracy: %f\" % lgs.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해석)\n",
    "- 분류 정확도가 86% 정도\n",
    "- TF-IDF는 85% 정도이므로 TF-IDF에 비해 Word2Vec이 성능이 다소 좋다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성된 로지스틱 회귀모델을 이용하여 평가 데이타 결과를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이타 파일 불러오기: 텍스트 형태 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_review = list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naturally film main themes mortality nostalgia...</td>\n",
       "      <td>\"12311_10\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie disaster within disaster film full great...</td>\n",
       "      <td>\"8348_2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie kids saw tonight child loved one point k...</td>\n",
       "      <td>\"5828_4\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afraid dark left impression several different ...</td>\n",
       "      <td>\"7186_2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accurate depiction small time mob life filmed ...</td>\n",
       "      <td>\"12128_7\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review          id\n",
       "0  naturally film main themes mortality nostalgia...  \"12311_10\"\n",
       "1  movie disaster within disaster film full great...    \"8348_2\"\n",
       "2  movie kids saw tonight child loved one point k...    \"5828_4\"\n",
       "3  afraid dark left impression several different ...    \"7186_2\"\n",
       "4  accurate depiction small time mob life filmed ...   \"12128_7\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이타도 학습 데이타와 동일하게 각 리뷰가 하나의 문자열로 이루어져 있으므로 \n",
    "- 평가 데이타도 각 단어의 리스트로 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이타도 word2vec으로 임베딩된 벡터값으로 만들어야 한다.\n",
    "- 평가 데이타에는 새롭게 word2vec 모델을 만들면 안되고, 학습 데이타 모델에 학습시킨 모델을 이용하여\n",
    "- 평가 데이타에 각 단어들을 벡터로 만들어 각 리뷰에 대한 특징값을 만든다.\n",
    "- 그런 후 이전에 정의했던 함수를 동일하게 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jinsuk kim\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성된 로지스틱회귀모델을 이용하여 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "print(test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예측한 결과값을 데이타프레임형태로 만들어 csv 파일로 저장\n",
    "- 각 데이타의 고유값 id와 결과값으로 구성\n",
    "- 캐글에 csv 파일 제출한 후 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
