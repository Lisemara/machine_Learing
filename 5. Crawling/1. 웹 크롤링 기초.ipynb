{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹 크롤링 기초\n",
    "### BeautifulSoup 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HTML 태그를 이용한 웹 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "<html>\n",
       "<body>\n",
       "<h1>웹 크롤링이란?</h1>\n",
       "<p>웹 페이지를 분석하는 것</p>\n",
       "<p>원하는 부분을 추출하는 것</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석하고 싶은 HTML \n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "      <h1>웹 크롤링이란?</h1>\n",
    "      <p>웹 페이지를 분석하는 것</p>\n",
    "      <p>원하는 부분을 추출하는 것</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup\n",
    "\n",
    "# # 원하는 부분 추출하기 \n",
    "# h1 = soup.html.body.h1\n",
    "# p1 = soup.html.body.p\n",
    "# p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# # 요소의 글자 출력하기 \n",
    "# print(\"h   = \" + h1.string)\n",
    "# print(\"p1  = \" + p1.string)\n",
    "# print(\"p2  = \" + p2.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 여러 개의 요소 추출하기 : find_all() 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>, <a href=\"http://www.goole.com\">google</a>]\n",
      "naver : http://www.naver.com\n",
      "daum : http://www.daum.net\n",
      "google : http://www.goole.com\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "      <ul>\n",
    "        <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "        <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "        <li><a href=\"http://www.goole.com\">google</a></li>\n",
    "      </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find_all() 메서드로 추출하기 \n",
    "links = soup.find_all(\"a\")\n",
    "print(links)\n",
    "\n",
    "# 링크 목록 출력하기 \n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \":\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CSS 선택자 사용하기\n",
    "- 1) soup.select(\"선택자\") : CSS 선택자로 요소 여러 개를 리스트로 추출\n",
    "- 2) soup.select_one(\"선택자\") : CSS 선택자로소 요소 하나를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 : 파이썬\n",
      "li :  파이썬 기초\n",
      "li :  데이타 분석\n",
      "li :  머신 러닝\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "# 분석 대상 HTML \n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <div id=\"subject\">\n",
    "      <h1>파이썬</h1>\n",
    "      <ul class=\"course\">\n",
    "        <li>파이썬 기초</li>\n",
    "        <li>데이타 분석</li>\n",
    "        <li>머신 러닝</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "</body> </html>\n",
    "\"\"\"\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# i) 타이틀 부분 추출하기 \n",
    "h1 = soup.select_one(\"div#subject > h1\").string\n",
    "print(\"h1 :\", h1)\n",
    "\n",
    "# ii) 목록 부분 추출하기 \n",
    "li_list = soup.select(\"div#subject > ul.course > li\")\n",
    "\n",
    "for li in li_list:\n",
    "    print(\"li : \", li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. url.request 모듈과 BeautifulSoup 조합하기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 16일(화) 오전은 중부지방(강원영동 제외)과 전라권에 비가 오겠습니다. 18일(목)은 제주도에 비가 오겠습니다.○ (기온) 이번 예보기간의 아침 기온은 -1~12도로 오늘(11일, 0~7도)과 비슷하거나 조금 높겠으나, 낮 기온은 10~19도로 오늘(11일, 13~21도)과 비슷하거나 조금 낮겠습니다.          한편, 낮 기온은 전국 대부분 지역에서 15도 이상으로 올라 포근하겠으나, 내륙을 중심으로 낮과 밤의 기온차가 크겠으니, 환절기 건강관리에 유의하기 바랍니다.○ (해상) 16일(화)은 동해중부해상에서 바람이 매우 강하게 불겠고, 물결이 2.0~4.0m로 매우 높게 일겠습니다.○ (주말전망) 13일(토)과 14일(일) 중부지방은 대체로 흐리고, 남부지방은 대체로 맑겠으나, 강원영동은 13일(토) 새벽까지 비 또는 눈이 오는 곳이 있겠습니다.               아침 기온은 0~8도, 낮 기온은 9~17도가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as request\n",
    "\n",
    "# 기상청 RSS\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\" \n",
    "\n",
    "# urlopen()으로 데이터 가져오기 \n",
    "res = request.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기 \n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 데이터 추출하기 \n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf.replace(\"<br />\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습 과제]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CSS 선택자와 find 메서드를 이용하여 아보카도 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<div id=\"main-goods\" role=\"page\">\n",
    "  <h1>과일과 야채</h1>\n",
    "  <ul id=\"fr-list\">\n",
    "    <li class=\"red green\" data-lo=\"ko\">사과</li>\n",
    "    <li class=\"purple\" data-lo=\"us\">포도</li>\n",
    "    <li class=\"yellow\" data-lo=\"us\">레몬</li>\n",
    "    <li class=\"yellow\" data-lo=\"ko\">오렌지</li>\n",
    "  </ul>\n",
    "  <ul id=\"ve-list\">\n",
    "    <li class=\"white green\" data-lo=\"ko\">무</li>\n",
    "    <li class=\"red green\" data-lo=\"us\">파프리카</li>\n",
    "    <li class=\"black\" data-lo=\"ko\">가지</li>\n",
    "    <li class=\"black\" data-lo=\"us\">아보카도</li>\n",
    "    <li class=\"white\" data-lo=\"cn\">연근</li>\n",
    "  </ul>\n",
    "</div>\n",
    "<body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아보카도\n",
      "아보카도\n",
      "아보카도\n",
      "아보카도\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# (1) CSS 선택자로 추출하기\n",
    "#ve-list > li:nth-child(4)\n",
    "print(soup.select(\"#ve-list > li\")[3].string)   \n",
    "print(soup.select(\"#ve-list > li.black\")[1].string) \n",
    "\n",
    "# # (2) find 메서드로 추출하기 \n",
    "cond = {\"data-lo\":\"us\", \"class\":\"black\"}\n",
    "print(soup.find(\"li\", cond).string)\n",
    "\n",
    "# # (3) find 메서드를 연속적으로 사용하기 \n",
    "print(soup.find(id=\"ve-list\").find(\"li\", cond).string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 정규 표현식으로 href에서 https인 것 추출하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '아버지', '아버지', '아버지']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall('아.?지', '아버지가 아버지께서 아버지와 아버지는')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t123@']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[a-z][0-9]{2,}@','test123@domain.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 'domain.co.kr', '.kr')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([a-z][a-z0-9]{2,})@(\\w{3,}([.][a-z]{2,})+)','test@domain.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.co', '.kr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([.][a-z]{2,})+?','.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('test', 'domain.co.kr', '.kr')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([a-z][a-z0-9]{2,})@(\\w{3,}([.][a-z]{2,})+)','test@domain.co.kr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식 연습 사이트 - https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 정규 표현식으로 href에서 https인 것 추출하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://example.com/test2\n",
      "https://example.com/test3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re                       # 정규 표현식 모듈\n",
    "\n",
    "html = \"\"\"\n",
    "<ul>\n",
    "  <li><a href=\"test1.com\">test1</li>\n",
    "  <li><a href=\"https://example.com/test2\">test2</li>\n",
    "  <li><a href=\"https://example.com/test3\">test3</li>\n",
    "  <li><a href=\"http://example.com/test4\">test4</li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 정규 표현식으로 href에서 https인 것 추출하기 \n",
    "li = soup.find_all(href=re.compile(r\"^https://\"))\n",
    "for e in li: \n",
    "    print(e.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 위키에 있는 윤동주 작가의 작품 목록 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 서시\n",
      "2 : 자화상\n",
      "3 : 소년\n",
      "4 : 눈 오는 지도\n",
      "5 : 돌아와 보는 밤\n",
      "6 : 병원\n",
      "7 : 새로운 길\n",
      "8 : 간판 없는 거리\n",
      "9 : 태초의 아침\n",
      "10 : 또 태초의 아침\n",
      "11 : 새벽이 올 때까지\n",
      "12 : 무서운 시간\n",
      "13 : 십자가\n",
      "14 : 바람이 불어\n",
      "15 : 슬픈 족속\n",
      "16 : 눈감고 간다\n",
      "17 : 또 다른 고향\n",
      "18 : 길\n",
      "19 : 별 헤는 밤\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib.request as request\n",
    "\n",
    "# 위키 윤동주 작가 페이지 \n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = request.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "a_list = soup.select(\"#mw-content-text > div > ul > li > ul > li\")\n",
    "\n",
    "for i, a in enumerate(a_list, start=1):\n",
    "    name = a.string\n",
    "    print(i, \":\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹에서 이미지 가져오기 - 바이너리 데이타 \n",
    "from urllib import request\n",
    "\n",
    "target = request.urlopen(\"https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTVjps%2FbtqIs8OTe9a%2FQK5RLftK1Sof1EfOxYBRx1%2Fimg.png\")\n",
    "output = target.read()\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바이너리 데이타를 쓰기 모드로 이미지 저장 - \"wb\"\n",
    "file = open(\"output.png\", \"wb\")\n",
    "file.write(output)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}